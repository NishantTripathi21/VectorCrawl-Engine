// processCrawledPages.js

import { chunkText } from "../processing/chunker.js";
import { getBatchEmbeddings } from "../processing/embedder.js";
import { storeVectors } from "../db/storeVectors.js";
import { v4 as uuidv4 } from "uuid";

export async function processCrawledPages(crawledPages, sessionId) {
  try {
    console.log("üß© Starting chunk + embed + store pipeline for session:", sessionId);

    // 1Ô∏è‚É£ Build chunks
    let allChunks = [];

    crawledPages.forEach((page) => {
      if (page.success && page.text) {
        const chunks = chunkText(page.text, page.url);
        allChunks.push(...chunks);
      }
    });

    console.log(`üß© Created ${allChunks.length} chunks`);

    if (allChunks.length === 0) {
      throw new Error("No chunks extracted from pages");
    }

    // 2Ô∏è‚É£ Extract raw text list
    const textList = allChunks.map((c) => c.chunk);

    // 3Ô∏è‚É£ Generate embeddings in batch
    console.log("üß† Generating batch embeddings...");
    const embeddings = await getBatchEmbeddings(textList);

    console.log("üì¶ Embeddings generated:", embeddings.length);

    // 4Ô∏è‚É£ Build Pinecone vector format
    const pineconeVectors = embeddings.map((embedding, i) => ({
      id: uuidv4(),
      values: embedding,
      metadata: {
        sessionId,                  // Added
        url: allChunks[i].url,
        seq: allChunks[i].seq,
        chunk: allChunks[i].chunk,
      },
    }));

    // 5Ô∏è‚É£ Store in Pinecone under this sessionId namespace
    console.log("üü¶ Storing vectors in Pinecone under namespace:", sessionId);
    await storeVectors(pineconeVectors, sessionId);

    console.log("üéâ Chunk + Embed + Pinecone storing DONE");

    return {
      success: true,
      chunksCreated: allChunks.length,
      vectorsStored: pineconeVectors.length,
    };

  } catch (err) {
    console.error("‚ùå processCrawledPages ERROR:", err);
    throw err;
  }
}
