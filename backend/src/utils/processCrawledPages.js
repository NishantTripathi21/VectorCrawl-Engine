// processCrawledPages.js

import { chunkText } from "../processing/chunker.js";
import { getBatchEmbeddings } from "../processing/embedder.js";
import { storeVectors } from "../db/storeVectors.js";
import { v4 as uuidv4 } from "uuid";

export async function processCrawledPages(crawledPages) {
  try {
    console.log("ğŸ§© Starting chunk + embed + store pipeline");

    // 1ï¸âƒ£ Build chunks
    let allChunks = [];

    crawledPages.forEach((page) => {
      if (page.success && page.text) {
        const chunks = chunkText(page.text, page.url);
        allChunks.push(...chunks);
      }
    });

    console.log(`ğŸ§© Created ${allChunks.length} chunks`);

    if (allChunks.length === 0) {
      throw new Error("No chunks extracted from pages");
    }

    // 2ï¸âƒ£ Extract raw text
    const textList = allChunks.map((c) => c.chunk);

    // 3ï¸âƒ£ Generate embeddings in batch
    console.log("ğŸ§  Generating batch embeddings...");
    const embeddings = await getBatchEmbeddings(textList);

    console.log("ğŸ“¦ Embeddings generated:", embeddings.length);

    // 4ï¸âƒ£ Build Pinecone vector format
    const pineconeVectors = embeddings.map((embedding, i) => ({
      id: uuidv4(),
      values: embedding,
      metadata: {
        url: allChunks[i].url,
        seq: allChunks[i].seq,
        chunk: allChunks[i].chunk,
      },
    }));

    // 5ï¸âƒ£ Store in Pinecone
    console.log("ğŸŸ¦ Storing vectors in Pinecone...");
    await storeVectors(pineconeVectors);

    console.log("ğŸ‰ Chunk + Embed + Pinecone storing DONE");

    return {
      success: true,
      chunksCreated: allChunks.length,
      vectorsStored: pineconeVectors.length,
    };

  } catch (err) {
    console.error("âŒ processCrawledPages ERROR:", err);
    throw err;
  }
}
